{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5VYIdCsnuVM0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import jax\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying To load T15 Data"
      ],
      "metadata": {
        "id": "aGQn3pFsusoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import h5py\n",
        "\n",
        "ls = []\n",
        "parent_folder = \"/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final\"\n",
        "if not os.path.exists(parent_folder):\n",
        "    print(\"Path not found!\")\n",
        "else:\n",
        "    for root, dirs, files in os.walk(parent_folder):\n",
        "        for file in files:\n",
        "            if file.endswith(\".hdf5\"):\n",
        "                file_path = os.path.join(root,file)\n",
        "                ls.append(file_path)"
      ],
      "metadata": {
        "id": "X7mFNpcPutZ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "yE5lDMrxuv9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243e610f-fc53-4508-de9d-d00703a97aaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.11/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.13/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.18/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.20/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.25/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.27/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.27/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.08.27/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.01/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.01/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.01/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.03/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.03/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.03/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.24/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.24/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.24/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.29/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.29/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.09.29/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.01/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.01/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.01/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.06/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.06/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.06/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.08/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.08/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.08/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.13/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.13/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.13/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.15/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.15/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.15/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.20/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.20/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.20/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.22/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.22/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.10.22/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.03/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.03/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.03/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.04/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.04/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.04/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.17/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.17/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.17/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.19/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.19/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.19/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.26/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.26/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.11.26/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.03/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.03/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.03/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.08/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.08/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.08/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.10/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.10/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.10/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.17/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.17/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.17/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.29/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.29/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2023.12.29/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.02.25/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.02.25/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.02.25/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.03/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.08/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.08/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.08/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.15/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.15/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.15/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.17/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.17/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.03.17/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.25/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.04.28/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.05.10/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.05.10/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.05.10/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.06.14/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.06.14/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.06.14/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.19/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.19/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.19/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.21/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.21/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.21/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.28/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.28/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2024.07.28/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.10/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.10/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.10/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.12/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.12/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.01.12/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.14/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.14/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.14/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.16/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.16/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.16/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.30/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.30/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.03.30/data_val.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.04.13/data_test.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.04.13/data_train.hdf5',\n",
              " '/mnt/c/Users/Siddh/Datasets/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final/t15.2025.04.13/data_val.hdf5']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets check whats within these hdf5 files..."
      ],
      "metadata": {
        "id": "RUI86XfvLVEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "sample_file = ls[0]\n",
        "\n",
        "with h5py.File(sample_file, 'r') as f:\n",
        "    print(f\"Keys in file: {list(f.keys())}\")\n",
        "\n",
        "    # Let's look at the shape of the data inside a key\n",
        "    for key in f.keys():\n",
        "        item = f[key]\n",
        "        if isinstance(item, h5py.Dataset):\n",
        "            print(f\"Key: {key} | Shape: {item.shape} | Type: {item.dtype}\")\n",
        "        else:\n",
        "            # print(f\"Key: {key} is a Group (folder)\")\n",
        "            continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "womj1wvk3qJ_",
        "outputId": "47607709-48f6-46cd-c7b8-8238b5ef6368"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in file: ['trial_0000', 'trial_0001', 'trial_0002', 'trial_0003', 'trial_0004', 'trial_0005', 'trial_0006', 'trial_0007', 'trial_0008', 'trial_0009', 'trial_0010', 'trial_0011', 'trial_0012', 'trial_0013', 'trial_0014', 'trial_0015', 'trial_0016', 'trial_0017', 'trial_0018', 'trial_0019', 'trial_0020', 'trial_0021', 'trial_0022', 'trial_0023', 'trial_0024', 'trial_0025', 'trial_0026', 'trial_0027', 'trial_0028', 'trial_0029', 'trial_0030', 'trial_0031', 'trial_0032', 'trial_0033', 'trial_0034', 'trial_0035', 'trial_0036', 'trial_0037', 'trial_0038', 'trial_0039', 'trial_0040', 'trial_0041', 'trial_0042', 'trial_0043', 'trial_0044', 'trial_0045', 'trial_0046', 'trial_0047', 'trial_0048', 'trial_0049', 'trial_0050', 'trial_0051', 'trial_0052', 'trial_0053', 'trial_0054', 'trial_0055', 'trial_0056', 'trial_0057', 'trial_0058', 'trial_0059', 'trial_0060', 'trial_0061', 'trial_0062', 'trial_0063', 'trial_0064', 'trial_0065', 'trial_0066', 'trial_0067', 'trial_0068', 'trial_0069', 'trial_0070', 'trial_0071', 'trial_0072', 'trial_0073', 'trial_0074', 'trial_0075', 'trial_0076', 'trial_0077', 'trial_0078', 'trial_0079', 'trial_0080', 'trial_0081', 'trial_0082', 'trial_0083', 'trial_0084', 'trial_0085', 'trial_0086', 'trial_0087', 'trial_0088', 'trial_0089', 'trial_0090', 'trial_0091', 'trial_0092', 'trial_0093', 'trial_0094', 'trial_0095', 'trial_0096', 'trial_0097', 'trial_0098', 'trial_0099', 'trial_0100', 'trial_0101', 'trial_0102', 'trial_0103', 'trial_0104', 'trial_0105', 'trial_0106', 'trial_0107', 'trial_0108', 'trial_0109', 'trial_0110', 'trial_0111', 'trial_0112', 'trial_0113', 'trial_0114', 'trial_0115', 'trial_0116', 'trial_0117', 'trial_0118', 'trial_0119', 'trial_0120', 'trial_0121', 'trial_0122', 'trial_0123', 'trial_0124', 'trial_0125', 'trial_0126', 'trial_0127', 'trial_0128', 'trial_0129', 'trial_0130', 'trial_0131', 'trial_0132', 'trial_0133', 'trial_0134', 'trial_0135', 'trial_0136', 'trial_0137', 'trial_0138', 'trial_0139', 'trial_0140', 'trial_0141', 'trial_0142', 'trial_0143', 'trial_0144', 'trial_0145', 'trial_0146', 'trial_0147', 'trial_0148', 'trial_0149', 'trial_0150', 'trial_0151', 'trial_0152', 'trial_0153', 'trial_0154', 'trial_0155', 'trial_0156', 'trial_0157', 'trial_0158', 'trial_0159', 'trial_0160', 'trial_0161', 'trial_0162', 'trial_0163', 'trial_0164', 'trial_0165', 'trial_0166', 'trial_0167', 'trial_0168', 'trial_0169', 'trial_0170', 'trial_0171', 'trial_0172', 'trial_0173', 'trial_0174', 'trial_0175', 'trial_0176', 'trial_0177', 'trial_0178', 'trial_0179', 'trial_0180', 'trial_0181', 'trial_0182', 'trial_0183', 'trial_0184', 'trial_0185', 'trial_0186', 'trial_0187', 'trial_0188', 'trial_0189', 'trial_0190', 'trial_0191', 'trial_0192', 'trial_0193', 'trial_0194', 'trial_0195', 'trial_0196', 'trial_0197', 'trial_0198', 'trial_0199', 'trial_0200', 'trial_0201', 'trial_0202', 'trial_0203', 'trial_0204', 'trial_0205', 'trial_0206', 'trial_0207', 'trial_0208', 'trial_0209', 'trial_0210', 'trial_0211', 'trial_0212', 'trial_0213', 'trial_0214', 'trial_0215', 'trial_0216', 'trial_0217', 'trial_0218', 'trial_0219', 'trial_0220', 'trial_0221', 'trial_0222', 'trial_0223', 'trial_0224', 'trial_0225', 'trial_0226', 'trial_0227', 'trial_0228', 'trial_0229', 'trial_0230', 'trial_0231', 'trial_0232', 'trial_0233', 'trial_0234', 'trial_0235', 'trial_0236', 'trial_0237', 'trial_0238', 'trial_0239', 'trial_0240', 'trial_0241', 'trial_0242', 'trial_0243', 'trial_0244', 'trial_0245', 'trial_0246', 'trial_0247', 'trial_0248', 'trial_0249', 'trial_0250', 'trial_0251', 'trial_0252', 'trial_0253', 'trial_0254', 'trial_0255', 'trial_0256', 'trial_0257', 'trial_0258', 'trial_0259', 'trial_0260', 'trial_0261', 'trial_0262', 'trial_0263', 'trial_0264', 'trial_0265', 'trial_0266', 'trial_0267', 'trial_0268', 'trial_0269', 'trial_0270', 'trial_0271', 'trial_0272', 'trial_0273', 'trial_0274', 'trial_0275', 'trial_0276', 'trial_0277', 'trial_0278', 'trial_0279', 'trial_0280', 'trial_0281', 'trial_0282', 'trial_0283', 'trial_0284', 'trial_0285', 'trial_0286', 'trial_0287']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shows that the HDF5 files are hierarchical i.e. Instead of one big block of data, your file is organized into \"Groups\" (which act like folders), where each group represents a single \"Trial\"\n",
        "\n",
        "##### > File Level: Contains ~288 folders (`trial_0000`, etc.).\n",
        "##### > Trial Level: Inside each `trial_XXXX` folder, you will find the actual Datasets (the neural arrays, the target text, etc.).\n",
        "\n",
        "### Let us look into a single trial"
      ],
      "metadata": {
        "id": "63anySMZLc4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "sample_file = ls[0]\n",
        "\n",
        "with h5py.File(sample_file, 'r') as f:\n",
        "    trial_group = f['trial_0000']\n",
        "\n",
        "    print(f\"--- Inspecting inside 'trial_0000' ---\")\n",
        "    print(f\"Keys: {list(trial_group.keys())}\")\n",
        "\n",
        "    # Loop through the items in this trial to see their shapes\n",
        "    for key in trial_group.keys():\n",
        "        data_item = trial_group[key]\n",
        "\n",
        "        # Check if it's actual data (Dataset) or another folder\n",
        "        if isinstance(data_item, h5py.Dataset):\n",
        "            # We want to see the shape (e.g., [Time, Channels]) and type\n",
        "            print(f\"  [DATASET] Name: {key:<20} | Shape: {data_item.shape} | Type: {data_item.dtype}\")\n",
        "\n",
        "            # If it's a small text label, let's print it to see what it says\n",
        "            if data_item.size < 10 and (data_item.dtype.kind in 'SUa'): # String/Unicode types\n",
        "                print(f\"            Value: {data_item[()]}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"  [GROUP]   Name: {key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gll8d8r1LYma",
        "outputId": "150c0b48-e23a-4cda-eb0c-f3bda3d6445c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Inspecting inside 'trial_0000' ---\n",
            "Keys: ['input_features', 'seq_class_ids', 'transcription']\n",
            "  [DATASET] Name: input_features       | Shape: (321, 512) | Type: float32\n",
            "  [DATASET] Name: seq_class_ids        | Shape: (500,) | Type: int32\n",
            "  [DATASET] Name: transcription        | Shape: (500,) | Type: int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Challenge: \"Trials inside Files\"\n",
        "### You cannot just pass the list of file paths to the generic PyTorch loader because one file contains multiple samples (trials). If you have 100 files and each has 200 trials, you actually have 20,000 samples. To tackle this we create a `Global Index Map`"
      ],
      "metadata": {
        "id": "OkFhp_FWLpmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "samples_index = []\n",
        "\n",
        "for file_path in ls:\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            # We just read the keys (trial names), we don't load the heavy data\n",
        "            trial_names = list(f.keys())\n",
        "            for t_name in trial_names:\n",
        "                samples_index.append((file_path, t_name))\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping broken file: {file_path}\")\n",
        "\n",
        "print(f\"Indexing complete!\")\n",
        "print(f\"Total files: {len(ls)}\")\n",
        "print(f\"Total individual trials (samples): {len(samples_index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdaT_jnPLm_X",
        "outputId": "dfce6802-fb45-4af7-ddc3-b7429eee49f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing complete!\n",
            "Total files: 127\n",
            "Total individual trials (samples): 10948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drobmvtgL8yG",
        "outputId": "72bae0c7-4d3b-46ff-c2ef-dbd0a9df2aa2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We load the given data into 3 different dataframes, which then we use later on as our reference dataframes"
      ],
      "metadata": {
        "id": "Sh8Ls_caBxjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ls = [ ... your list of file paths ... ]\n",
        "\n",
        "manifest = []\n",
        "\n",
        "print(f\"Scanning {len(ls)} files using FILENAME logic...\")\n",
        "\n",
        "for file_path in ls:\n",
        "    try:\n",
        "        # Extract filename and folder info\n",
        "        file_name = os.path.basename(file_path)\n",
        "        path_parts = file_path.replace('\\\\', '/').split('/')\n",
        "        session_date = next((p for p in path_parts if p.startswith('t15.')), \"Unknown\")\n",
        "\n",
        "        # 1. DETERMINE TYPE BY FILENAME\n",
        "        if \"train\" in file_name:\n",
        "            split_type = \"train\"\n",
        "        elif \"val\" in file_name:\n",
        "            split_type = \"validation\"\n",
        "        elif \"test\" in file_name:\n",
        "            split_type = \"test\"\n",
        "        else:\n",
        "            split_type = \"unknown\"\n",
        "\n",
        "        # 2. VERIFY CONTENTS (Do labels exist?)\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            if len(f.keys()) == 0:\n",
        "                continue\n",
        "\n",
        "            first_trial = list(f.keys())[0]\n",
        "            group = f[first_trial]\n",
        "\n",
        "            has_labels = 'transcription' in group\n",
        "            n_trials = len(f.keys())\n",
        "\n",
        "            manifest.append({\n",
        "                'file_path': file_path,\n",
        "                'session_date': session_date,\n",
        "                'filename': file_name,\n",
        "                'split': split_type,      # 'train', 'val', 'test'\n",
        "                'has_labels': has_labels, # True/False\n",
        "                'n_trials': n_trials\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "# --- SUMMARY REPORT ---\n",
        "df = pd.DataFrame(manifest)\n",
        "df.to_csv(\"t15_split_manifest.csv\", index=False)\n",
        "\n",
        "print(\"\\n--- FINAL SPLIT REPORT ---\")\n",
        "print(df.groupby(['split', 'has_labels'])['n_trials'].sum())\n",
        "\n",
        "# CRITICAL CHECK: Does validation data have labels?\n",
        "val_labels = df[df['split'] == 'validation']['has_labels'].all()\n",
        "if val_labels:\n",
        "    print(\"\\n[SUCCESS] All Validation files have labels! You can calculate accuracy immediately.\")\n",
        "else:\n",
        "    print(\"\\n[WARNING] Some Validation files are missing labels. Check the CSV.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aABFOVwZ71et",
        "outputId": "52f1f1c0-6611-4662-b23b-94f80444fdc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning 127 files using FILENAME logic...\n",
            "\n",
            "--- FINAL SPLIT REPORT ---\n",
            "split       has_labels\n",
            "test        False         1450\n",
            "train       True          8072\n",
            "validation  True          1426\n",
            "Name: n_trials, dtype: int64\n",
            "\n",
            "[SUCCESS] All Validation files have labels! You can calculate accuracy immediately.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the Master Manifest we created in the previous step\n",
        "manifest_path = \"t15_split_manifest.csv\"\n",
        "\n",
        "if not os.path.exists(manifest_path):\n",
        "    print(f\"Error: '{manifest_path}' not found. Please run the Classification Script first.\")\n",
        "else:\n",
        "    # Read the full list\n",
        "    full_df = pd.read_csv(manifest_path)\n",
        "\n",
        "    # 2. Filter into 3 DataFrames\n",
        "    # We create copies (.copy()) so we can modify them later without warnings\n",
        "    train_df = full_df[full_df['split'] == 'train'].copy()\n",
        "    val_df   = full_df[full_df['split'] == 'validation'].copy()\n",
        "    test_df  = full_df[full_df['split'] == 'test'].copy()\n",
        "\n",
        "    # 3. Verify the Counts\n",
        "    print(\"--- DATASET SPLIT REPORT ---\")\n",
        "    print(f\"TRAIN Set : {len(train_df)} files\")\n",
        "    print(f\"VAL Set   : {len(val_df)} files\")\n",
        "    print(f\"TEST Set  : {len(test_df)} files\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"TOTAL     : {len(full_df)} files\")\n",
        "\n",
        "    # 4. Quick Sanity Check\n",
        "    # Ensure Train/Val actually have labels (should be True)\n",
        "    train_has_labels = train_df['has_labels'].all()\n",
        "    val_has_labels = val_df['has_labels'].all()\n",
        "\n",
        "    if train_has_labels and val_has_labels:\n",
        "        print(\"\\n[OK] Integrity Check: All Train and Validation files have labels.\")\n",
        "    else:\n",
        "        print(\"\\n[WARNING] Some Train/Val files are missing labels! Check your manifest.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZKe_QRKBzKP",
        "outputId": "014195dd-2da3-40fc-f512-a62035ab4a7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DATASET SPLIT REPORT ---\n",
            "TRAIN Set : 45 files\n",
            "VAL Set   : 41 files\n",
            "TEST Set  : 41 files\n",
            "------------------------------\n",
            "TOTAL     : 127 files\n",
            "\n",
            "[OK] Integrity Check: All Train and Validation files have labels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a character vocabulary set, we'll use the train_df as reference to build it and identify every unique character in it"
      ],
      "metadata": {
        "id": "NQ4M52rMDqz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "\n",
        "train_files = train_df['file_path'].to_list()\n",
        "\n",
        "print(f\"Scanning {len(train_files)} training files to build the keyboard...\")\n",
        "unique_chars = set()\n",
        "\n",
        "for file_path in train_files:\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            for key in f.keys():\n",
        "                group = f[key]\n",
        "\n",
        "                # Check for the sentence text in attributes\n",
        "                if 'sentence_label' in group.attrs:\n",
        "                    sentence = group.attrs['sentence_label']\n",
        "\n",
        "                    # Convert bytes to string if needed\n",
        "                    if isinstance(sentence, bytes):\n",
        "                        sentence = sentence.decode('utf-8')\n",
        "\n",
        "                    # Add every character to our set\n",
        "                    for char in sentence:\n",
        "                        unique_chars.add(char)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file: {e}\")\n",
        "# --- FORMATTING THE VOCAB ---\n",
        "# Sort the list so 'a' always comes before 'b'\n",
        "sorted_chars = sorted(list(unique_chars))\n",
        "\n",
        "# Create the map: Char -> Number\n",
        "# We start at 1 because 0 is usually reserved for the \"Blank\" token in CTC Loss\n",
        "char_to_int = {char: idx + 1 for idx, char in enumerate(sorted_chars)}\n",
        "int_to_char = {idx + 1: char for idx, char in enumerate(sorted_chars)}\n",
        "\n",
        "# Add the special CTC Blank Token\n",
        "char_to_int['<BLANK>'] = 0\n",
        "int_to_char[0] = '<BLANK>'\n",
        "\n",
        "# Save it!\n",
        "vocab_data = {\n",
        "    'char_to_int': char_to_int,\n",
        "    'int_to_char': int_to_char,\n",
        "    'n_classes': len(char_to_int) # This tells us how many output neurons we need\n",
        "}\n",
        "\n",
        "with open(\"t15_vocab.json\", \"w\") as f:\n",
        "    json.dump(vocab_data, f, indent=4)\n",
        "\n",
        "print(\"\\n--- KEYBOARD BUILT ---\")\n",
        "print(f\"Found {len(sorted_chars)} unique characters.\")\n",
        "print(f\"Total Model Output Size: {len(char_to_int)} (including Blank)\")\n",
        "print(f\"Characters: {''.join(sorted_chars)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvqq82ieCcbX",
        "outputId": "0e1ed87a-241f-42db-c675-fd453a4ba101"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning 45 training files to build the keyboard...\n",
            "\n",
            "--- KEYBOARD BUILT ---\n",
            "Found 62 unique characters.\n",
            "Total Model Output Size: 63 (including Blank)\n",
            "Characters:  !',-.;?ABCDEFGHIJKLMNOPQRSTUVWYZ[]abcdefghijklmnopqrstuvwxyzâ€™\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "28mT0w_HAC0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "H25X7XEpDsTM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_granular_manifest(file_list, split_name):\n",
        "    samples = []\n",
        "    print(f\"Indexing {len(file_list)} files for {split_name} set...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        try:\n",
        "            with h5py.File(file_path, 'r') as f:\n",
        "                # We iterate through EVERY key (trial_0000, trial_0001...)\n",
        "                for trial_name in f.keys():\n",
        "                    samples.append({\n",
        "                        'file_path': file_path,\n",
        "                        'trial_name': trial_name,\n",
        "                        'split': split_name\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping broken file {file_path}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(samples)\n",
        "    print(f\"--> Found {len(df)} total samples for {split_name}.\")\n",
        "    return df\n",
        "\n",
        "# 1. Get list of files from your existing manifest (or however you loaded them)\n",
        "# Assuming 'train_df' and 'val_df' from your previous code still exist and contain file paths:\n",
        "train_files = train_df['file_path'].tolist()\n",
        "val_files = val_df['file_path'].tolist()\n",
        "\n",
        "# 2. Create the new \"Granular\" DataFrames\n",
        "train_df = create_granular_manifest(train_files, \"Train\")\n",
        "val_df = create_granular_manifest(val_files, \"Validation\")\n",
        "\n",
        "# 3. Check the result\n",
        "print(\"\\n--- NEW MANIFEST CHECK ---\")\n",
        "print(f\"Old Train Size: {len(train_files)} files\")\n",
        "print(f\"New Train Size: {len(train_df)} trials (Use this one!)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8CRs3Ka8qdE",
        "outputId": "f2dad56d-1460-448e-bdc2-657157676fd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing 45 files for Train set...\n",
            "--> Found 8072 total samples for Train.\n",
            "Indexing 41 files for Validation set...\n",
            "--> Found 1426 total samples for Validation.\n",
            "\n",
            "--- NEW MANIFEST CHECK ---\n",
            "Old Train Size: 45 files\n",
            "New Train Size: 8072 trials (Use this one!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class T15Dataset(Dataset):\n",
        "    def __init__(self, df, vocab_path=\"t15_vocab.json\", smooth_sigma=4.0, clip_val=5.0, augment=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.smooth_sigma = smooth_sigma\n",
        "        self.clip_val = clip_val\n",
        "        self.augment = augment\n",
        "\n",
        "        # Load Vocabulary\n",
        "        with open(vocab_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            self.char_to_int = data['char_to_int']\n",
        "            self.blank_token = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        result = []\n",
        "        for char in text:\n",
        "            if char in self.char_to_int:\n",
        "                result.append(self.char_to_int[char])\n",
        "        return torch.LongTensor(result)\n",
        "\n",
        "    def preprocess_neural(self, neural_data):\n",
        "        # 1. ARTIFACT REJECTION (Optional but recommended)\n",
        "        # Mask high-amplitude artifacts (>50) before they ruin statistics\n",
        "        sanity_mask = np.abs(neural_data) > 50\n",
        "        neural_data[sanity_mask] = 0.0\n",
        "\n",
        "        # 2. Identify Dead Channels\n",
        "        ch_std = np.std(neural_data, axis=0)\n",
        "        dead_channels = ch_std < 0.01\n",
        "\n",
        "        # 3. Z-SCORE NORMALIZATION\n",
        "        mean = np.mean(neural_data, axis=0)\n",
        "        std = ch_std\n",
        "        std[std == 0] = 1.0 # Prevent divide by zero\n",
        "        normalized = (neural_data - mean) / std\n",
        "\n",
        "        # 4. Kill Dead Channels\n",
        "        normalized[:, dead_channels] = 0.0\n",
        "\n",
        "        # 5. CLIP OUTLIERS\n",
        "        normalized = np.clip(normalized, -self.clip_val, self.clip_val)\n",
        "\n",
        "        # 6. SMOOTHING\n",
        "        smoothed = gaussian_filter1d(normalized, sigma=self.smooth_sigma, axis=0)\n",
        "\n",
        "        return torch.from_numpy(smoothed).float()\n",
        "\n",
        "    def apply_augmentation(self, neural_tensor):\n",
        "        noise_level = 0.1\n",
        "        noise = torch.randn_like(neural_tensor) * noise_level\n",
        "        active_mask = (neural_tensor.abs().sum(dim=1, keepdim=True) > 0)\n",
        "        return neural_tensor + noise * active_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # --- FIXED LOGIC HERE ---\n",
        "        row = self.df.iloc[idx]\n",
        "        file_path = row['file_path']\n",
        "        trial_name = row['trial_name'] # Get the specific trial name (e.g., 'trial_0042')\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            # Open the SPECIFIC trial group directly\n",
        "            group = f[trial_name]\n",
        "\n",
        "            # Load & Preprocess Input\n",
        "            raw_neural = group['input_features'][:]\n",
        "            neural_tensor = self.preprocess_neural(raw_neural)\n",
        "\n",
        "            # Augmentation\n",
        "            if self.augment:\n",
        "                neural_tensor = self.apply_augmentation(neural_tensor)\n",
        "\n",
        "            # Load Target (Labels)\n",
        "            if 'sentence_label' in group.attrs:\n",
        "                sentence = group.attrs['sentence_label']\n",
        "                if isinstance(sentence, bytes):\n",
        "                    sentence = sentence.decode('utf-8')\n",
        "                target_tensor = self.text_to_int(sentence)\n",
        "            else:\n",
        "                # Fallback for validation files that use 'transcription' dataset instead of attributes\n",
        "                target_tensor = torch.from_numpy(group['transcription'][:]).long()\n",
        "\n",
        "            input_len = neural_tensor.shape[0]\n",
        "            target_len = len(target_tensor)\n",
        "\n",
        "        return neural_tensor, target_tensor, input_len, target_len"
      ],
      "metadata": {
        "id": "qbHBTPLaAiqX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating two instances of this class. One for training (with noise) and one for validation (pure)."
      ],
      "metadata": {
        "id": "oAnUcczub_qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Training Dataset (With Augmentation enabled)\n",
        "train_dataset = T15Dataset(\n",
        "    df=train_df,\n",
        "    vocab_path=\"t15_vocab.json\",\n",
        "    augment=True  # <--- Step 9: Enabled\n",
        ")\n",
        "\n",
        "# 2. Validation Dataset (Clean data only)\n",
        "val_dataset = T15Dataset(\n",
        "    df=val_df,\n",
        "    vocab_path=\"t15_vocab.json\",\n",
        "    augment=False # <--- Disabled\n",
        ")\n",
        "\n",
        "print(f\"Train Size: {len(train_dataset)}\")\n",
        "print(f\"Val Size:   {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzKu9Z2zb6wS",
        "outputId": "ecfa6467-3143-4d09-d3c1-ca4ef541ce2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Size: 8072\n",
            "Val Size:   1426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `ctc_collate_fn` returns 4 things, not just 2: It essentially just performing padding of the translational sequences\n",
        "\n",
        "1. `padded_neural`: The rectangular input data (for the GPU).\n",
        "\n",
        "2. `padded_targets`: The rectangular labels (for the GPU).\n",
        "\n",
        "3. `input_lens`: A list saying [200, 500, 1000] (The real lengths).\n",
        "\n",
        "4. `target_lens`: A list saying [2, 11, 35] (The real character counts)."
      ],
      "metadata": {
        "id": "APCrAgQ0cFkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def ctc_collate_fn(batch):\n",
        "    neural_tensors, target_tensors, input_lens, target_lens = zip(*batch)\n",
        "    flat_targets = torch.cat(target_tensors, dim=0)\n",
        "    padded_neural = pad_sequence(neural_tensors, batch_first=True, padding_value=0.0)\n",
        "    # padded_targets = pad_sequence(target_tensors, batch_first=True, padding_value=-1)\n",
        "    input_lens = torch.tensor(input_lens, dtype=torch.long)\n",
        "    target_lens = torch.tensor(target_lens, dtype=torch.long)\n",
        "    return padded_neural, flat_targets, input_lens, target_lens\n",
        "train_dataset = T15Dataset(train_df, vocab_path=\"t15_vocab.json\", augment=True)\n",
        "val_dataset   = T15Dataset(val_df, vocab_path=\"t15_vocab.json\", augment=False)"
      ],
      "metadata": {
        "id": "Q90_mphucBao"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,       # ALWAYS shuffle training data\n",
        "    collate_fn=ctc_collate_fn,\n",
        "    num_workers=2,      # Uses multi-core CPU to load files faster\n",
        "    pin_memory=True     # Speeds up transfer to GPU\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,      # Never shuffle validation (keep order consistent)\n",
        "    collate_fn=ctc_collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "wHfBdQiTcIBA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab first batch\n",
        "inputs, targets, in_lens, out_lens = next(iter(train_loader))\n",
        "\n",
        "print(\"\\n--- BATCH INSPECTION ---\")\n",
        "print(f\"Input Shape (Batch, Max_Time, 512): {inputs.shape}\")\n",
        "print(f\"Target Shape (Batch, Max_Seq_Len):  {targets.shape}\")\n",
        "print(f\"Sample Input Lengths: {in_lens[:5].tolist()}\")\n",
        "\n",
        "# Check for padding (should see zeros at the end of the first sample if it's shorter than max)\n",
        "if inputs.shape[1] > in_lens[0]:\n",
        "    print(\"\\n[OK] Padding detected (zeros found at end of sequence).\")\n",
        "else:\n",
        "    print(\"\\n[NOTE] First sequence was the longest, or batch sizes match exactly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT3WHP-mcJ93",
        "outputId": "f3bed5ba-ead6-4f76-eedf-6956e15584c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- BATCH INSPECTION ---\n",
            "Input Shape (Batch, Max_Time, 512): torch.Size([16, 1878, 512])\n",
            "Target Shape (Batch, Max_Seq_Len):  torch.Size([541])\n",
            "Sample Input Lengths: [785, 1322, 1878, 1374, 876]\n",
            "\n",
            "[OK] Padding detected (zeros found at end of sequence).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class T15Decoder(nn.Module):\n",
        "    def __init__(self, input_dim = 512, num_classes = 63, hidden_dim = 1024, num_layers = 5, dropout = 0.3):\n",
        "        super(T15Decoder, self).__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU() #Better for timeseries than ReLU\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size = hidden_dim,\n",
        "            hidden_size = hidden_dim,\n",
        "            num_layers = num_layers,\n",
        "            batch_first = True,\n",
        "            dropout = dropout,\n",
        "            bidirectional= True #Required for past and future context\n",
        "        )\n",
        "        self.output = nn.Linear(hidden_dim*2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.transpose(0, 1)\n",
        "        x = self.input_fc(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        rnn_out, _ = self.gru(x)\n",
        "        logits = self.output(rnn_out)\n",
        "\n",
        "        return F.log_softmax(logits, dim=2)"
      ],
      "metadata": {
        "id": "svd_r1XlcNmj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = T15Decoder(\n",
        "    input_dim=512,\n",
        "    num_classes=63,\n",
        "    hidden_dim=512\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created on {device}. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "inputs, targets, in_lens, out_lens = next(iter(train_loader))\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "with torch.no_grad():\n",
        "    predictions = model(inputs) # For now this is mainly baloney predictions passed with random logits\n",
        "\n",
        "print(\"\\n--- ARCHITECTURE CHECK ---\")\n",
        "print(f\"Input Shape:  {inputs.shape}  (Batch, Time, 512)\")\n",
        "print(f\"Output Shape: {predictions.shape} (Batch, Time, 63)\")\n",
        "\n",
        "if inputs.shape[1] == predictions.shape[1]:\n",
        "    print(\"[SUCCESS] Shapes align. The engine is running.\")\n",
        "else:\n",
        "    print(f\"[ERROR] Time mismatch! In: {inputs.shape[1]}, Out: {predictions.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njFmW0M5dxUg",
        "outputId": "1030dea6-11be-4974-9dc5-192d57b1ba60"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created on cuda. Parameters: 22,378,047\n",
            "\n",
            "--- ARCHITECTURE CHECK ---\n",
            "Input Shape:  torch.Size([16, 1143, 512])  (Batch, Time, 512)\n",
            "Output Shape: torch.Size([16, 1143, 63]) (Batch, Time, 63)\n",
            "[SUCCESS] Shapes align. The engine is running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Model (The Stammerer):\n",
        "The GRU RNN. It outputs probabilities for every single millisecond of audio.\n",
        "Because audio is longer than text, it outputs repeats and blanks.Model Output: \"hhh...eeee...lll...lll...[blank]...oooo\"\n",
        "### The Decoder (The Stenographer):\n",
        "This is the algorithm that cleans up that raw output. It looks at the probabilities and applies the \"CTC Rules\":Rule 1: Collapse repeated characters (ll $\\to$ l).Rule 2: Remove the \"blank\" token (a special character the model learns to output when there is silence or between duplicate letters).Final Output: \"hello\""
      ],
      "metadata": {
        "id": "h7vSgVdJ6MkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We're going with the `CTC-loss` and `AdamW` Optimizer"
      ],
      "metadata": {
        "id": "oERZv2k2p--p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "3NFhW3CFeqHK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So currently we're going to go with the Greedy Search Decoder, primarily as a method to save hardware resources, considering we're on a bit of a RAM Crunch, once we get hold of the RTX 4070 PC's with more VRAM, we will change it to a better Beam-Search Decoder that takes the top-k entire contextual sentences instead of just the top most probable word, making it a more smarter choice and a probable boost in performance later on, as of now to get a working model ready for training, we'll stick to the former"
      ],
      "metadata": {
        "id": "qn4gg_ACqEQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_ctc_decode(\n",
        "    log_probs,\n",
        "    input_lengths,\n",
        "    blank_id=0,\n",
        "    collapse_repeated=True\n",
        "):\n",
        "    \"\"\"\n",
        "    log_probs: (T, B, V)\n",
        "    input_lengths: (B,)\n",
        "    returns: List[str] of length B\n",
        "    \"\"\"\n",
        "    argmax = torch.argmax(log_probs, dim=-1)  # (T, B)\n",
        "\n",
        "    decoded_strings = []\n",
        "\n",
        "    for b in range(argmax.shape[1]):\n",
        "        seq = argmax[:input_lengths[b], b].tolist()\n",
        "\n",
        "        decoded = []\n",
        "        prev = None\n",
        "        for token in seq:\n",
        "            if token == blank_id:\n",
        "                prev = token\n",
        "                continue\n",
        "            if collapse_repeated and token == prev:\n",
        "                continue\n",
        "            decoded.append(token)\n",
        "            prev = token\n",
        "\n",
        "        decoded_strings.append(\"\".join(int_to_char[t] for t in decoded))\n",
        "\n",
        "    return decoded_strings\n",
        "\n",
        "\n",
        "def reconstruct_targets(flat_targets, target_lengths):\n",
        "    targets = []\n",
        "    offset = 0\n",
        "    for length in target_lengths:\n",
        "        seq = flat_targets[offset:offset + length].tolist()\n",
        "        targets.append(\"\".join(int_to_char[t] for t in seq))\n",
        "        offset += length\n",
        "    return targets\n"
      ],
      "metadata": {
        "id": "B11Z3tQwqBNT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "metadata": {
        "id": "nERkak-brF0G"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "GRAD_CLIP = 1.0\n",
        "LOG_INTERVAL = 50\n",
        "\n",
        "ctc_loss_fn = nn.CTCLoss(\n",
        "    blank=0,\n",
        "    reduction=\"mean\",\n",
        "    zero_infinity=True\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqGdgJj5rLNN",
        "outputId": "4270ce6c-0e54-4ca9-fe3c-277fbf5b9be4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T15Decoder(\n",
              "  (input_fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (activation): GELU(approximate='none')\n",
              "  (gru): GRU(512, 512, num_layers=5, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (output): Linear(in_features=1024, out_features=63, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import Levenshtein  # Optional: For calculating Character Error Rate (CER) later\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, epoch_idx):\n",
        "    model.train()  # Enable dropout\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Initialize Progress Bar\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch_idx} [TRAIN]\")\n",
        "\n",
        "    for batch_idx, (inputs, flat_targets, input_lens, target_lens) in enumerate(pbar):\n",
        "        # 1. Move Data to GPU\n",
        "        inputs = inputs.to(device)\n",
        "        flat_targets = flat_targets.to(device)\n",
        "        input_lens = input_lens.to(device)\n",
        "        target_lens = target_lens.to(device)\n",
        "\n",
        "        # 2. Zero Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 3. Forward Pass\n",
        "        # Model Output: (Batch, Time, Classes)\n",
        "        log_probs = model(inputs)\n",
        "\n",
        "        # 4. Transform for CTC Loss\n",
        "        # CTCLoss expects: (Time, Batch, Classes)\n",
        "        log_probs_p = log_probs.permute(1, 0, 2)\n",
        "\n",
        "        # 5. Calculate Loss\n",
        "        # flat_targets is 1D, target_lens tells it how to chop it up\n",
        "        loss = criterion(log_probs_p, flat_targets, input_lens, target_lens)\n",
        "\n",
        "        # 6. Backward Pass & Step\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping (Prevents exploding gradients in RNNs)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # 7. Update Logs\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': running_loss / (batch_idx + 1)})\n",
        "\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, criterion, device, int_to_char):\n",
        "    model.eval()  # Disable dropout\n",
        "    running_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_targets = []\n",
        "\n",
        "    # We only need a progress bar if validation takes a long time\n",
        "    # pbar = tqdm(loader, desc=\"[VAL]\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, flat_targets, input_lens, target_lens) in enumerate(loader):\n",
        "            inputs = inputs.to(device)\n",
        "            flat_targets = flat_targets.to(device)\n",
        "            input_lens = input_lens.to(device)\n",
        "            target_lens = target_lens.to(device)\n",
        "\n",
        "            # Forward\n",
        "            log_probs = model(inputs)\n",
        "            log_probs_p = log_probs.permute(1, 0, 2)\n",
        "\n",
        "            # Loss\n",
        "            loss = criterion(log_probs_p, flat_targets, input_lens, target_lens)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Decode ONLY the first batch for visual inspection (save time)\n",
        "            if batch_idx == 0:\n",
        "                # Decode Predictions (Greedy)\n",
        "                # Note: We need to pass log_probs (Batch, Time, Classes) to your decoder\n",
        "                decoded_text = greedy_ctc_decode(log_probs, input_lens)\n",
        "                val_preds.extend(decoded_text)\n",
        "\n",
        "                # Decode Targets (Ground Truth)\n",
        "                target_text = reconstruct_targets(flat_targets, target_lens)\n",
        "                val_targets.extend(target_text)\n",
        "\n",
        "    return running_loss / len(loader), val_preds, val_targets"
      ],
      "metadata": {
        "id": "UZ81M7ParNmd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- HYPERPARAMETERS ---\n",
        "NUM_EPOCHS = 20\n",
        "BEST_VAL_LOSS = float('inf')\n",
        "SAVE_PATH = \"best_t15_model.pth\"\n",
        "\n",
        "print(f\"Starting training on {len(train_dataset)} trials...\")\n",
        "print(f\"Validating on {len(val_dataset)} trials...\")\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "    # 1. Train\n",
        "    train_loss = train_one_epoch(model, train_loader, ctc_loss_fn, optimizer, DEVICE, epoch)\n",
        "\n",
        "    # 2. Validate\n",
        "    val_loss, preds, targets = validate(model, val_loader, ctc_loss_fn, DEVICE, int_to_char)\n",
        "\n",
        "    # 3. Print Stats\n",
        "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # 4. Show us what it learned! (Print first 2 samples)\n",
        "    print(\"-\" * 50)\n",
        "    for i in range(min(2, len(preds))):\n",
        "        print(f\"Target: {targets[i]}\")\n",
        "        print(f\"Pred  : {preds[i]}\")\n",
        "        print(\".\" * 20)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 5. Save Checkpoint\n",
        "    if val_loss < BEST_VAL_LOSS:\n",
        "        BEST_VAL_LOSS = val_loss\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        print(f\">>> New Best Model Saved! (Loss: {val_loss:.4f})\")\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu8ZcyCPr-Sn",
        "outputId": "4748175f-833f-47c5-cf9c-1780728d600d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on 8072 trials...\n",
            "Validating on 1426 trials...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [TRAIN]:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        | 124/505 [21:08<48:24,  7.62s/it, loss=8.38]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLojo2aQsAeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}